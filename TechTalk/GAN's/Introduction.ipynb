{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network(GAN's) ##\n",
    "\n",
    "GAN's are deep neural nets contains of two neural nets pittying against one another thus why it's called as **Adversarial Network**.\n",
    "\n",
    "\n",
    "## Invented By ##\n",
    "GANs were introduced in a paper by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014. Referring to GANs, Facebook’s AI research director Yann LeCun called adversarial training “the most interesting idea in the last 10 years in ML.”\n",
    "\n",
    "## Idea ##\n",
    "\n",
    "The idea is to train simultaneously two networks and competes again each other. \n",
    "\n",
    "1. Discriminator - Let's denote it D(y). It takes an input image abd outputs a scalar that indicates whether the input image is real or not.  In simple terms,\n",
    "                If D(y)=0,\n",
    "                    then Real\n",
    "                else\n",
    "                   D(y)=1. then Fake\n",
    "    The closer D(Y) to zero, the real the image gets.\n",
    " \n",
    "2. Generator - Let's denote it G(z). z is the randomly sampled vector in a simple distribution. The role of Generator is to produce an input image similar to D(Y) from the randomly Sampled vector. The goal is to train D(Y) to take into a  right shape. During Training D is shown an real image to adjust its parameter to make it lower. well G(z) will produce its own input image from the randomly noise vector thinking that it is the real image and adjust it parameter to make D(G(z)) as larger.  G(z) will produce the input image from the random distribution and then D will shown the image produced by G. If the D identifies the image is fake, the gradient of image (Y) will be passed back to G for each sample it produces. In other words, the Generator will try to minimize the output of D while Discriminator tries larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underlying Idea ##\n",
    "\n",
    "### Interpreting images from Random Sample from a probability distribution ###\n",
    "\n",
    "![randomImage](images/completion.compressed.gif)\n",
    "\n",
    "Probability distribution plays a major role in image processing and deep learning. For example, lets take an above picture that we know some values and want to complete the missing value. Just pose it as a maximization problem where we search over all of the  possible missing values. however, applying this method in practise is easy to recover for simple distributions, its difficult and often interactable for complex distribution over images. The complexity partly comes from [conditional dependencies], the value of one pixel completely depends on another. Also maximizing the PDF for an image might be difficult and often intractable non-convex optimization problem.\n",
    "\n",
    "\n",
    "### Overall Layer of GAN ###\n",
    "![ganschema](images/gan_schema.png)\n",
    "\n",
    "The both models are multilayer perceptron. To learn generator's distribution data p<sub>(g) over data X, we define a prior on input noise variable p<sub>z(Z), then represents mapping data into higher dimensional space. G(z;theta(g)) where G is the differentiable function represented by multilayer perceptron with parameters theta(g).  \n",
    "    \n",
    "D(x, theta(d)) is the scalar value which comes from the second layer of perceptron. It describes probability of x rather the p<sub>(g). We simultaneously train generator G to minimize\n",
    "                1 - log(D(G(z))\n",
    "\n",
    "\n",
    "The loss function will be\n",
    "\n",
    "![ganLossFunction](images/gan_loss.png)\n",
    "\n",
    "\n",
    "\n",
    "In the inner loop of algorithm D is trained to discriminate samples from data, converging \n",
    "\n",
    "            D<sup>*(x) = p<sub>(data)(x) / (p<sub>(data)(x) + p<sub>(g)(x)\n",
    "\n",
    "After an update to G, gradient of D has guided G(z) to flow regions that are more likely classified to be as data.(ie.., Discriminator will help generator to help its distribution of the data).\n",
    "\n",
    "After several steps of training, if G and D achieved its enough capacity, they will reach a point at which both cannot improve because p<sub>(g) = p<sub>data(X). The discriminator is unable to differentiate between two distributions.\n",
    "\n",
    "![ganDistribution](images/gan_distribution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture ###\n",
    "\n",
    "![DiscriminatorUpdateweights](images/discriminator_weight_update.png)\n",
    "\n",
    "![GeneratorUpdateWeights](images/generator_weight_updates.png)\n",
    "\n",
    "Another way to look at the GAN setup is that the Discriminator is trying to guide the Generator how the real image looks like. Generator eventually learns its distribution and tries to start generating real-looking images. \n",
    "\n",
    "#### Mathematics ####\n",
    "\n",
    "Let X be our true dataset and Z be the normal distributed noise. let p(z) from the latent space Z. G and D are differentiable functions of generative and discriminator network. D(x), is the data coming from real dataset X. Where D is trying to maximize the probability of **log(D(x))** and train G to minimize **log(1-D(G(z))**.\n",
    "\n",
    "\n",
    "![ganinitialimagesample](images/gan_mathematics.png)\n",
    "\n",
    "Inorder to prove that the image produced by Generator is same as the discriminator we need to know **Kullback-Leibler divergence**. This theorem tells us how exactly the probability distribution of one image say N is different from other.\n",
    "\n",
    "\n",
    "#### Generator Architecture ####\n",
    "![GeneratorArchitectureImage](images/generator_network.png)\n",
    "\n",
    "In above architecture, we use dense layer 4x4x1024 to create a dense vector out of 100d vector. once we get 1024 4x4 maps, we do upsampling using a series of Transposed convolutionals which it will upscale the image size, each operation doubles the size of the image and halves the number of maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Comparisons ##\n",
    "#### Different between Variational AutoEncoders and GAN's ####\n",
    "\n",
    "Both GAN's and Autoencoders are generative models, which means they learn a distribution of model rather then its density. \n",
    "\n",
    "Autoencoders learn a given distribution comparing its input to produce output. Autoencoders are good for learning hidden representation. It's bad for creating a new data because it learns the generalized distribution of data and producing images on that data produce blurry image.\n",
    "\n",
    "#### What are Transpose Convolutions? ####\n",
    "\n",
    "Transpose convolutions are way to scale up the image size. In general convolutions we downsize the image 4x4 to 2x2. In this we are doing 2x2 and 4x4.\n",
    "![upscaling](images/upscaling.png)\n",
    "\n",
    "\n",
    "In the case of Discriminator is just a normal CNN classifier where it says whether the image is **fake or not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
