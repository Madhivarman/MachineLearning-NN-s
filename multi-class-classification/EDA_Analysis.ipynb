{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0yUrF26bVI7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcpeGyCBbzxT"
   },
   "source": [
    "#### Read the data Directly from the Drive ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJbPFefgb4tq"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFgmLY-Vbb4l"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG-z_gMqb7L1"
   },
   "outputs": [],
   "source": [
    "def readURL(sharedPath, filename):\n",
    "    fluff, id = sharedPath.split('=')\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile(filename)  \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wK3rp_tcY-D"
   },
   "outputs": [],
   "source": [
    "trainLink = 'link_to_the_data'\n",
    "evalLink = 'link_to_the_data'\n",
    "\n",
    "traindf = readURL(trainLink, 'train.csv')\n",
    "evaldf = readURL(evalLink, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wntJkUDEcxCR"
   },
   "source": [
    "#### Summary information  and Meta Data####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "nBL6x9wScqkg",
    "outputId": "dad7f8ea-5e5f-43c1-e9f8-da5def300b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575907 entries, 0 to 575906\n",
      "Data columns (total 2 columns):\n",
      "titles    575907 non-null object\n",
      "labels    575907 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "traindf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "N2u6WRu-c06k",
    "outputId": "d081ff5a-e6d6-4a55-a54e-6448449cc1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143977 entries, 0 to 143976\n",
      "Data columns (total 1 columns):\n",
      "titles    143977 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "evaldf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqmfwGz0dG7i"
   },
   "source": [
    "#### Problem Statement ####\n",
    "\n",
    "Build a classifier to classify product titles. We want to classify an input product title into categories.\n",
    "Training data to classify this product into categories is provided with the problem. The categories\n",
    "assigned in the training data are the only categories to be considered.\n",
    "Also provided is a set of product titles where your classifier should predict the relevant labels. We\n",
    "will internally evaluate the classifier on the prediction on the test data set.\n",
    "Please provide all the relevant pre-processing, model development and tuning code.\n",
    "Few points\n",
    "1. Itâ€™s a tab separated file. The first column is the title, the second column contains labels.\n",
    "2. There are multiple labels in the 2 nd column but for this exercise you can consider only one\n",
    "label. If you want you can set it up as a multi label classification problem but its ok if you do\n",
    "not.\n",
    "3. Simplify the problem if need be.\n",
    "4. In evaluation please generate a probability score for each label as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0W0DU6dhdTpF"
   },
   "source": [
    "#### work on the dataset ####\n",
    "\n",
    "the column label schema is Array. We need to separate the arrays as product Categories and Labels. There will be one categories each product titles but multiple labels (or) no labels at all for the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM2ARJ1xdpQV"
   },
   "source": [
    "#### custom function to separate the lables ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blvgVtlDdDWd"
   },
   "outputs": [],
   "source": [
    "def separateLabels(df):\n",
    "    #copy the dataset\n",
    "    df_copy = df.copy()\n",
    "    #first of all remove all square brackes\n",
    "    df_copy['labels'] = df_copy['labels'].apply(lambda x: x.replace('[','').replace(']',''))\n",
    "    df['category'] = df_copy['labels'].apply(lambda x: x.split(\",\")[0].replace(\"'\",''))\n",
    "    df['macro_category'] = df_copy['labels'].apply(lambda x: x.split(\",\")[1:])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqsbPrm0d4lt"
   },
   "outputs": [],
   "source": [
    "process_traindf = separateLabels(traindf) #dataframe separated into category and macro_category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXZ49HDEzCTq"
   },
   "source": [
    "\n",
    "#### Now convert the macro category into multilabel classification problem ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddOjB2tpzQPa"
   },
   "outputs": [],
   "source": [
    "def convertIntoMultilabel(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['macro_category'] = df_copy['macro_category'].apply(lambda x: \"no_label\" if len(x) == 0 else x)\n",
    "    visited_label = ['no_label'] #list to store macro_labels\n",
    "    for macro_lables in df['macro_category']:\n",
    "        if macro_lables == 'no_label':\n",
    "          pass\n",
    "        else:\n",
    "          for label in macro_lables:\n",
    "              if label in visited_label:\n",
    "                  continue\n",
    "              else:\n",
    "                  visited_label.append(label)\n",
    "    \n",
    "    print(\"total macro category length is:{}\".format(len(visited_label)))\n",
    "\n",
    "    one_hot_value = [] #list to append after doing one-hot encode\n",
    "\n",
    "    for nums, labels in enumerate(df_copy['macro_category']):\n",
    "      #console log\n",
    "      if nums % 50000 == 0 and nums != 0:\n",
    "          print(\"finished one hot encoding for n={}\".format(nums))\n",
    "\n",
    "      one_hot_encode = [0]*len(visited_label)\n",
    "      if labels == 'no_label':\n",
    "        one_hot_encode[0] = 1\n",
    "        one_hot_value.append(one_hot_encode)\n",
    "      else:\n",
    "          for indi_labels in labels:\n",
    "              #get the index of the label and assign true\n",
    "              index = visited_label.index(indi_labels)\n",
    "              one_hot_encode[index]=1\n",
    "          \n",
    "          one_hot_value.append(one_hot_encode)\n",
    "      \n",
    "\n",
    "    return one_hot_value, visited_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "909lG_cgzhhu",
    "outputId": "d8f76efd-3b42-4d65-9911-2a3278c7a500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total macro category length is:177\n",
      "finished one hot encoding for n=50000\n",
      "finished one hot encoding for n=100000\n",
      "finished one hot encoding for n=150000\n",
      "finished one hot encoding for n=200000\n",
      "finished one hot encoding for n=250000\n",
      "finished one hot encoding for n=300000\n",
      "finished one hot encoding for n=350000\n",
      "finished one hot encoding for n=400000\n",
      "finished one hot encoding for n=450000\n",
      "finished one hot encoding for n=500000\n",
      "finished one hot encoding for n=550000\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoding, columns = convertIntoMultilabel(process_traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD8jKuhdjjyB"
   },
   "outputs": [],
   "source": [
    "column_rename = [x.replace(\"'\",'').replace(\" \",'') for x in columns]\n",
    "one_hot_df = pd.DataFrame(one_hot_encoding, columns=column_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6xbPzx4sx4-J",
    "outputId": "fbd7a159-4026-44b0-8912-b92d3b5e3729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the one hot encoding dataframe:(575907, 177)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the one hot encoding dataframe:{}\".format(one_hot_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJAw7ZMV6CQ_"
   },
   "source": [
    "#### concat one hot encoding dataframe + product dataframe ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PBaWICb65yxK",
    "outputId": "c8a36419-036d-4b38-d0c8-bab972c100df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575907, 179)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df = process_traindf[['titles', 'category']]\n",
    "\n",
    "##now concatenate via axis 1\n",
    "product_df = pd.concat([product_df, one_hot_df], axis=1)\n",
    "\n",
    "## dataset size is more than 786.5+ MB\n",
    "## as the column size increases w.r.t to same number of rows\n",
    "## size of the dataset will also increases\n",
    "product_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "TCc1iiEX6v0i",
    "outputId": "e8824fee-8847-47ab-bde1-38395f836ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575907 entries, 0 to 575906\n",
      "Columns: 179 entries, titles to tiloil\n",
      "dtypes: int64(177), object(2)\n",
      "memory usage: 786.5+ MB\n"
     ]
    }
   ],
   "source": [
    "product_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wd_IQ1eE7PRj"
   },
   "source": [
    "#### Processing the Titles ####\n",
    "\n",
    "Removing unwanted words from the dataset.\n",
    "\n",
    "  1. Our text preprocessing will include the following steps:\n",
    "  2. Convert all text to lower case.\n",
    "  3. Replace REPLACE_BY_SPACE_RE symbols by space in text.\n",
    "  4. Remove symbols that are in BAD_SYMBOLS_RE from text.\n",
    "  5. Remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "YofxE2t963Uk",
    "outputId": "7101d611-92c2-47ad-f8db-49f160eed2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanInputData(text):\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^a-z #+_]') #removing numbers from the dataset\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clN9Y65G8n8s"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df['title'] = process_traindf['titles'].apply(cleanInputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qW7dmgD1BiDj"
   },
   "source": [
    "#### Problem Approach ####\n",
    "\n",
    "1. For now, consider only multi-class problem.\n",
    "2. Check what is the maximum number of words in the titles column.\n",
    "3. Check the class count in the dataset. If the dataset is highly imbalanced for any of the particular label, do necessary steps to ensure that the model is stable.\n",
    "4. Convert the data into vectors (or) sequence of integers. Truncate and the pad the input sequence all to be in the same length for modeling\n",
    "5. Convert the targetLabel(categorical column) into numerical records for training\n",
    "6.  Do Train and Test Split.\n",
    "7.  Start Training a model, store all callbacks and return the probability metrics for evaluation dataset\n",
    "\n",
    "\n",
    "For reference, consider these two articles\n",
    "\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f - For Skipgram, reasoning for specific keywords for specific categories\n",
    "\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17 - For LSTM Model\n",
    "\n",
    "https://medium.com/@nitinpanwar98/text-classification-using-machine-learning-with-code-65a8491d389f - Majority Voting Scheme\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/ - Training neural net on Multi label Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zD7D0ShU26m3"
   },
   "source": [
    "Steps to do\n",
    " 1. Vectorize consumer complaints text, by turning each text into either a sequence of integers or into a vector.\n",
    " 2. Limit the data set to the top 50000 words. (for now, we are doing top 50000 words)\n",
    " 3. Set the max number of words in each complaint at 316."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_M2c68AOAD6Y",
    "outputId": "e10be6c9-c6e7-4b83-9d26-5094dd34474e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y3648MBB3Fvc",
    "outputId": "b91e1ebd-e85f-4eab-dfee-d7f05a6823e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20109 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 24000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 316\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(product_df['titles'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bcA0_Jnl3J7H",
    "outputId": "e78489d3-1f2d-480f-f830-7babaff6bf6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of target variable:254\n"
     ]
    }
   ],
   "source": [
    "total_target_var = product_df['category'].unique().tolist()\n",
    "print(\"total number of target variable:{}\".format(len(total_target_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTP6jFSs3SrC"
   },
   "source": [
    "### Check if the classes are lineary separable ###\n",
    "The classifiers and learning algorithms can not directly process the text documents in their original form, as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. Therefore, during the preprocessing step, the texts are converted to a more manageable representation.\n",
    "\n",
    "One common approach for extracting features from text is to use the bag of words model: a model where for each document, a complaint narrative in our case, the presence (and often the frequency) of words is taken into consideration, but the order in which they occur is ignored.\n",
    "\n",
    "Checking the words that are most important to the category.\n",
    "\n",
    "Warning: feeding the entire dataset to TfIdfVectorizer won't fit in the memory. So,feeding the subset of the each class and check the most common words for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDIiKn9G3NY4"
   },
   "outputs": [],
   "source": [
    "def getSubset(df):\n",
    "    #get the copy of the dataframe\n",
    "    df_copy = df\n",
    "    subsetdf = pd.DataFrame()\n",
    "    for num, label in enumerate(df['category'].unique().tolist()):\n",
    "        #get the dataset\n",
    "        label_df = df_copy[df_copy['category'] == label]\n",
    "        label_df = label_df[['titles', 'category']]\n",
    "        #get only 10000 from each dataset\n",
    "        if label_df.shape[0] <= 500:\n",
    "            subsetdf = pd.concat([subsetdf, label_df], axis=0)\n",
    "        else:\n",
    "            label_df = label_df.iloc[:500]\n",
    "            subsetdf = pd.concat([subsetdf, label_df], axis=0)\n",
    "        \n",
    "        if num % 50 == 0 and num != 0:\n",
    "            print(\"finished getting subset for '{}' category\".format(num))\n",
    "    \n",
    "    return  subsetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "rBIve9r03YID",
    "outputId": "749dd964-699e-4699-c6ee-18438abe25aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished getting subset for '50' category\n",
      "finished getting subset for '100' category\n",
      "finished getting subset for '150' category\n",
      "finished getting subset for '200' category\n",
      "finished getting subset for '250' category\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21125, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_pdf = getSubset(product_df)\n",
    "subset_pdf = subset_pdf.drop_duplicates()\n",
    "subset_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7y-bkhEk3aBs",
    "outputId": "2c59ce5d-fe0b-4f21-937c-e51f0e955a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21125, 5716)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words='english')\n",
    "feature_vector = tfidf_vectorizer.fit_transform(subset_pdf.titles).toarray()\n",
    "labels = subset_pdf.category\n",
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFgp1Y823dQU"
   },
   "outputs": [],
   "source": [
    "np.save(\"featureVector.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ZifFyU-_368p",
    "outputId": "40ed48d7-ab27-462e-ef09-340dddab2c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t       clustr_train_data.csv  sample_data\n",
      "cluster_eval_data.csv  featureVector.npy\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Zo48FMU4C2l"
   },
   "source": [
    "### finding most correlated words ###\n",
    "Now, each of 25351 titles is represented by 7666 features, representing the tf-idf score for different unigrams and bigrams. We can use sklearn.feature_selection.chi2 to find the terms that are the most correlated with each of the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "myeMEma7380b",
    "outputId": "03c017e7-8aa8-4297-ccf0-46364f776c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataset:(21125, 2)\n"
     ]
    }
   ],
   "source": [
    "subset_pdf['category_id'] = subset_pdf['category'].factorize()[0]\n",
    "subset_pdf_required = subset_pdf[['category','category_id']]\n",
    "print(\"shape of the dataset:{}\".format(subset_pdf_required.shape))\n",
    "#category to id values\n",
    "category_to_id = dict(subset_pdf_required.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9SKO3nHV-6Fm",
    "outputId": "e66cc07a-6b21-43ae-f1df-8d00ee1fcff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'canned food'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(category_to_id.keys())[list(category_to_id.values()).index(48)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "n6PBam084HaI",
    "outputId": "4d9e4c7b-eb44-4c72-bafb-306c43e4812f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Writing Unigrams and Bigrams for 50 categories\n",
      "Finished Writing Unigrams and Bigrams for 100 categories\n",
      "Finished Writing Unigrams and Bigrams for 150 categories\n",
      "Finished Writing Unigrams and Bigrams for 200 categories\n",
      "Finished Writing Unigrams and Bigrams for 250 categories\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "labels = subset_pdf_required['category_id']\n",
    "N = 2\n",
    "num = 1\n",
    "\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "\n",
    "    if num % 50 == 0 and num != 0:\n",
    "        print(\"Finished Writing Unigrams and Bigrams for {} categories\".format(num))\n",
    "\n",
    "    features_ch2 = chi2(feature_vector, labels == category_id)\n",
    "    indices = np.argsort(features_ch2[0])\n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    with open(\"words_grams_to_category.txt\", \"a\") as fp:\n",
    "        fp.write(\"## Product:{}\".format(list(category_to_id.keys())[list(category_to_id.values()).index(category_id)]))\n",
    "        fp.write(\"\\n\")\n",
    "        fp.write(\".......... Mosts Correlated Unigrams:\\n{}\".format('\\n'.join(unigrams[-N:])))\n",
    "        fp.write(\".......... Most Correlated Bigrams: \\n {}\".format('\\n'.join(bigrams[-N:])))\n",
    "        fp.write(\"\\n\\n\")\n",
    "        fp.write('****************************************')\n",
    "        fp.write(\"\\n\")\n",
    "        fp.close()\n",
    "    \n",
    "    num += 1 #increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "-vlz00IP4J5A",
    "outputId": "3e97fc16-7a95-46ed-a386-6567482ed74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json\t       clustr_train_data.csv  sample_data\n",
      "cluster_eval_data.csv  featureVector.npy      words_grams_to_category.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xz7WehwA9Ku0"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"words_grams_to_category.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qs4KPuqhBoPH"
   },
   "source": [
    "#### checking some conditions ####\n",
    "\n",
    "If we check the most correlated words for the categories in file **words_grams_to_category.txt** we can see for some class, the words are correctly captured. For Example - \n",
    "\n",
    "```\n",
    "basmati rice\n",
    "daawat\n",
    "basmati.......... Most Correlated Bigrams: \n",
    " kohinoor basmati\n",
    "basmati rice\n",
    "```\n",
    "\n",
    "If the bigrams are unique to identify these specific keywords then traditional Machine Learning Algorithms are able to capture these sepecific patterns and able to identify the classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGxhtoLLHaTR"
   },
   "source": [
    "#### Train and Test Split ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "rq6RPlwbIx0t",
    "outputId": "40cb95b9-67ec-4560-87b9-f7b7110764f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['titles', 'category', 'no_label', 'beverages', 'insectrepellent',\n",
       "       'pestrepellents', 'dairy', 'make-up', 'papad', 'appalam',\n",
       "       ...\n",
       "       'kitchenware', 'nipple', 'pista', 'masurdal', 'lentils', 'flossers',\n",
       "       'luggagebags', 'gingellyoil', 'tiloil', 'category_id'],\n",
       "      dtype='object', length=180)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert category into id's\n",
    "product_df_copy = product_df.copy()\n",
    "product_df_copy = product_df_copy.drop_duplicates()\n",
    "product_df_copy['category_id'] = product_df_copy['category'].factorize()[0]\n",
    "product_df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkSi5MQOBysK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(product_df_copy['titles'], product_df_copy['category_id'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jSoaHKivJd12",
    "outputId": "234b6bdf-e14c-4eb6-94a6-180be8017254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: Train Data(66473,), Train Label(66473,)\n",
      "Test Shape: Test Data(22158,), Test Label(22158,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: Train Data{}, Train Label{}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test Shape: Test Data{}, Test Label{}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkvfmy_xJqs1"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() #count Vectorizer\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer() #tfidf vectorizer\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#create a classifier\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoWvrFNRKG2-"
   },
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "q9LhJhi3PhTp",
    "outputId": "b98213fe-66f1-454e-c15e-b901e755f031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2265             cream kakao wafer biscuit loacker g\n",
       "12800      pedigree daily food adult dogs vegetarian\n",
       "65063              heracles olive oil pomace ltr tin\n",
       "28503                   danone milk toned ltr carton\n",
       "195510    nestle cerelac wheat apple stage gm carton\n",
       "320766      udupi ruchi kerala parata instant mix gm\n",
       "359448                               glucon kg plain\n",
       "410174            danish premium mutton curry cut gm\n",
       "418491                      yeos hot chilli sauce ml\n",
       "315688                  hamdard rooh afza sharbat ml\n",
       "Name: titles, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPElLM8zMXof"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for test_instance in X_test:\n",
    "    preds = clf.predict(count_vect.transform([test_instance]))\n",
    "    predictions.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7d6z4qVaSch1",
    "outputId": "18a891fd-f511-4419-9ab7-e3874f4c453f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy:0.5482895568192075\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "acc = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"MNB accuracy:{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPDWDroHUBMm"
   },
   "source": [
    "#### Observation ####\n",
    "\n",
    "Now, if we try to print metrics for all the classes, it will throw an error. Because in our dataset there are some class where instances are very low.(eg.., 1-10). To know the model stability we need equal distribution of the all classes in the train and test datasets. \n",
    "\n",
    "**Removing classes** that are very low in number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IytO5yisxHyJ"
   },
   "outputs": [],
   "source": [
    "def removeClassFromDataset(df, classes):\n",
    "    df = df[df['category'] != classes]\n",
    "    return df\n",
    "\n",
    "#removing classes that are lesser then 10 instances\n",
    "def removingLessClass(uniqueClass, df, n=10):\n",
    "    #removing class\n",
    "    classes_to_be_removed = []\n",
    "    for classes in uniqueClass:\n",
    "        class_df = df[df['category'] == classes]\n",
    "        if class_df.shape[0] <= 10:\n",
    "            classes_to_be_removed.append(classes)\n",
    "    \n",
    "    print(\"number of classes that have lesser than 10 instances:N={}\".format(len(classes_to_be_removed)))\n",
    "\n",
    "    for classes in classes_to_be_removed:\n",
    "        df = removeClassFromDataset(df, classes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWr9VjEd164H"
   },
   "outputs": [],
   "source": [
    "unique_brand = product_df_copy['category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iRyih7Ydx6V6",
    "outputId": "2523ab02-6445-4181-bb2b-26372a7d5a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes that have lesser than 10 instances:N=108\n"
     ]
    }
   ],
   "source": [
    "major_classes_df = removingLessClass(unique_brand, product_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kh2Ei9dQ4S-N",
    "outputId": "d3901074-2e24-4c67-9829-4abec9db2e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining class after removing minority class:146\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining class after removing minority class:{}\".format(len(major_classes_df['category'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBEMueZY444Z"
   },
   "source": [
    "### Splitting Train and Test ###\n",
    "\n",
    "Splitting the dataset into train and test set with equal manner. 70% of data in train data and 30% of data in test set for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNtUW7pS42kv"
   },
   "outputs": [],
   "source": [
    "def splitTrainAndTest(df):\n",
    "    #train and test dataframe\n",
    "    traindf, testdf = pd.DataFrame(), pd.DataFrame()\n",
    "    unique_class = df.category.unique().tolist()\n",
    "    \n",
    "    #iterate\n",
    "    for indi_class in unique_class:\n",
    "        subdf = df[df['category'] == indi_class]\n",
    "        n_rows = subdf.shape[0]\n",
    "        #split 70, 30 percent\n",
    "        train_percentage = int(float(n_rows * 70)/100)\n",
    "        train_subdf = subdf.iloc[:train_percentage]\n",
    "        test_subdf = subdf.iloc[train_percentage:]\n",
    "\n",
    "        #append to the dataset\n",
    "        traindf = pd.concat([traindf, train_subdf], axis=0)\n",
    "        testdf = pd.concat([testdf, test_subdf], axis=0)\n",
    "    \n",
    "\n",
    "    return traindf, testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9XHCdtt42no"
   },
   "outputs": [],
   "source": [
    "train, test = splitTrainAndTest(major_classes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "S3GZbaQa7YwZ",
    "outputId": "b9b65d42-0e23-47c5-9944-4a62b3ca51b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe shape:(61642, 180)\n",
      "test dataframe shape:(26511, 180)\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataframe shape:{}\".format(train.shape))\n",
    "print(\"test dataframe shape:{}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Un1sH-IV7hhc",
    "outputId": "11ec2f9e-a734-4d03-fd92-9baf1fcabcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of classes in the train dataset:146\n",
      "total number of classes in the test dataset:146\n"
     ]
    }
   ],
   "source": [
    "#check all class are present in the dataset\n",
    "print(\"total number of classes in the train dataset:{}\".format(len(train['category'].unique().tolist())))\n",
    "print(\"total number of classes in the test dataset:{}\".format(len(test['category'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKzBDMYy70Yj"
   },
   "source": [
    "#### Model Function ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5o9KdiPy7hkl"
   },
   "outputs": [],
   "source": [
    "def modelTrain(model, train, test):\n",
    "    count_vect = CountVectorizer() #count Vectorizer\n",
    "    tfidf_transformer = TfidfTransformer() #tfidf vectorizer\n",
    "\n",
    "    X_train_counts = count_vect.fit_transform(train['titles'])\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "    y_train = train['category_id']\n",
    "    y_test = test['category_id']\n",
    "\n",
    "    #create a classifier\n",
    "    clf = model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    print(\"Model finished Training.......................\")\n",
    "\n",
    "    #predictions\n",
    "    predictions = []\n",
    "\n",
    "    for test_instance in test['titles']:\n",
    "        preds = clf.predict(count_vect.transform([test_instance]))\n",
    "        predictions.append(preds) #append to the prediction\n",
    "    \n",
    "    return clf, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NSDqmvhk9G9l",
    "outputId": "365aea7f-9cc1-4ba2-e5b3-e30035fd151d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model finished Training.......................\n"
     ]
    }
   ],
   "source": [
    "mnb_clf, mnb_preds = modelTrain(MultinomialNB(), train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UUESQ53U-dDq",
    "outputId": "1d7ca4bd-14a9-407f-f39b-afd7aca2a93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Accuracy:0.5381162536305685\n"
     ]
    }
   ],
   "source": [
    "mnb_acc = metrics.accuracy_score(mnb_preds, test['category_id'])\n",
    "print(\"MNB Accuracy:{}\".format(mnb_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YNpVbGf7-dG0",
    "outputId": "92190fb8-2409-4e68-e85e-6df03cd189bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                     branded grocery       0.39      0.83      0.53      4023\n",
      "                              snacks       0.68      0.45      0.54       912\n",
      "                           home care       0.65      0.72      0.68      2276\n",
      "                   breakfast cereals       0.73      0.36      0.48       680\n",
      "              sweets & confectionery       0.27      0.04      0.07       674\n",
      "                           cosmetics       0.71      0.25      0.37      1056\n",
      "                        ready-to-eat       0.00      0.00      0.00        67\n",
      "                 fruits & vegetables       0.91      0.79      0.84       907\n",
      "                              fryums       0.00      0.00      0.00         6\n",
      "                           beverages       0.50      0.58      0.54       790\n",
      "                        instant food       1.00      0.00      0.01       236\n",
      "                             staples       0.75      0.54      0.63      1107\n",
      "                           chocolate       0.62      0.82      0.70       935\n",
      "                       personal care       0.44      0.92      0.60      2637\n",
      "                          deodorants       0.70      0.35      0.46       362\n",
      "                          edible oil       0.58      0.25      0.35       774\n",
      "                              spices       0.71      0.72      0.71       703\n",
      "                          healthcare       0.69      0.14      0.23       328\n",
      "                       otc medicines       1.00      0.03      0.05        77\n",
      "                           hair care       0.80      0.05      0.10       372\n",
      "                            biscuits       0.70      0.56      0.63       439\n",
      "                               dairy       0.69      0.40      0.51       457\n",
      "                           baby care       0.68      0.44      0.53       386\n",
      "                              bakery       0.83      0.51      0.63       440\n",
      "                           spreads**       0.86      0.14      0.25       290\n",
      "                              coffee       0.83      0.54      0.66       298\n",
      "                 dried fruits & nuts       0.75      0.27      0.39       381\n",
      "       cleaning agents & accessories       1.00      0.03      0.05       252\n",
      "                         frozen food       0.00      0.00      0.00        94\n",
      "                               cream       0.00      0.00      0.00        65\n",
      "                       ready-to-cook       0.81      0.38      0.51       401\n",
      "                               soaps       0.89      0.07      0.14       325\n",
      "                                eggs       0.77      0.90      0.83       448\n",
      "                            pet care       0.90      0.20      0.32        92\n",
      "                               water       0.54      0.68      0.60       546\n",
      "                            lip balm       0.00      0.00      0.00         8\n",
      "                        cold storage       0.00      0.00      0.00        71\n",
      "                    detergent powder       0.00      0.00      0.00       114\n",
      "                  ayurvedic products       0.00      0.00      0.00        49\n",
      "                              sweets       1.00      0.01      0.01       156\n",
      "                             pickles       0.78      0.64      0.71       472\n",
      "                       sanitary pads       0.00      0.00      0.00         6\n",
      "                                milk       0.00      0.00      0.00        94\n",
      "                       fresh chicken       0.78      0.33      0.47       129\n",
      "          frozen fruits & vegetables       1.00      0.05      0.10        92\n",
      "              stationery accessories       0.00      0.00      0.00         5\n",
      "                         canned food       0.95      0.21      0.35       183\n",
      "                               olive       0.00      0.00      0.00         5\n",
      "                    other condiments       0.00      0.00      0.00        51\n",
      "                           fragrance       0.00      0.00      0.00        10\n",
      "                 shaving accessories       0.00      0.00      0.00        90\n",
      "                          toothpaste       0.00      0.00      0.00         6\n",
      "                               bread       0.00      0.00      0.00        45\n",
      "                              squash       0.00      0.00      0.00         6\n",
      "                       instant mixes       0.00      0.00      0.00        26\n",
      "               shampoo & conditioner       0.00      0.00      0.00         7\n",
      "                      sanitary needs       0.00      0.00      0.00        34\n",
      "                       ground coffee       0.00      0.00      0.00         5\n",
      "                    organic products       0.00      0.00      0.00        14\n",
      "                           oral care       0.88      0.04      0.07       179\n",
      "                               honey       0.00      0.00      0.00         5\n",
      "                             cashews       0.00      0.00      0.00         5\n",
      "                       health drinks       0.00      0.00      0.00        72\n",
      "                             namkeen       0.00      0.00      0.00        17\n",
      "                              lotion       0.00      0.00      0.00        10\n",
      "                                salt       0.00      0.00      0.00        55\n",
      "                      frozen seafood       0.00      0.00      0.00        16\n",
      "                           nutrition       0.00      0.00      0.00        32\n",
      "                         conditioner       0.00      0.00      0.00        12\n",
      "                                 bun       0.00      0.00      0.00         4\n",
      "                          vegetables       0.00      0.00      0.00        43\n",
      "                               chips       0.00      0.00      0.00         8\n",
      "               multi-purpose cleaner       0.00      0.00      0.00         5\n",
      "                        wafer sticks       0.00      0.00      0.00         7\n",
      "                            desserts       0.00      0.00      0.00        11\n",
      "                        fruit juices       0.00      0.00      0.00        10\n",
      "                         soft drinks       0.00      0.00      0.00         6\n",
      "                           skin care       0.00      0.00      0.00         9\n",
      "                           face mask       0.00      0.00      0.00         6\n",
      "                        basmati rice       0.00      0.00      0.00        10\n",
      "                           face wash       0.00      0.00      0.00         7\n",
      "                                cake       0.00      0.00      0.00         9\n",
      "                         lime pickle       0.00      0.00      0.00         6\n",
      "                            dog food       0.00      0.00      0.00         4\n",
      "                       energy drinks       0.00      0.00      0.00         6\n",
      "                              yogurt       0.00      0.00      0.00         4\n",
      "                          vermicelli       0.00      0.00      0.00         5\n",
      "                  hair removal cream       0.00      0.00      0.00         7\n",
      "                       shaving cream       0.00      0.00      0.00        10\n",
      "                           pain balm       0.00      0.00      0.00         5\n",
      "                       cooking pasta       0.00      0.00      0.00         6\n",
      "                           olive oil       0.00      0.00      0.00         6\n",
      "                     dishwash liquid       0.00      0.00      0.00         6\n",
      "               agricultural products       0.00      0.00      0.00         5\n",
      "                          hair color       0.00      0.00      0.00         7\n",
      "                           body wash       0.00      0.00      0.00         6\n",
      "                               brush       0.00      0.00      0.00         4\n",
      "                                rusk       0.00      0.00      0.00         4\n",
      "                         after shave       0.00      0.00      0.00         5\n",
      "                       air freshener       0.00      0.00      0.00         4\n",
      "                     dishwash powder       0.00      0.00      0.00         4\n",
      "                        garbage bags       0.00      0.00      0.00         5\n",
      "                            pastries       0.00      0.00      0.00        21\n",
      "                              pulses       0.00      0.00      0.00        13\n",
      "                          mayonnaise       0.00      0.00      0.00         6\n",
      "                              fruits       0.00      0.00      0.00        20\n",
      "                        concentrates       0.00      0.00      0.00         6\n",
      "                              blades       0.00      0.00      0.00        12\n",
      "                        dishwash bar       0.00      0.00      0.00         4\n",
      "                         chewing gum       0.00      0.00      0.00         8\n",
      "                             popcorn       0.00      0.00      0.00         7\n",
      "                       floor cleaner       0.00      0.00      0.00         8\n",
      "                              muesli       0.00      0.00      0.00         7\n",
      "                         fabric care       0.00      0.00      0.00         5\n",
      "                           baby food       0.00      0.00      0.00        19\n",
      "                         food colour       0.00      0.00      0.00         4\n",
      "                                ghee       0.00      0.00      0.00         6\n",
      "                             almonds       0.00      0.00      0.00         4\n",
      "                                 tea       0.00      0.00      0.00        21\n",
      "                              chikki       0.00      0.00      0.00         7\n",
      "                      cream biscuits       0.00      0.00      0.00         6\n",
      "                        mango pickle       0.00      0.00      0.00         6\n",
      "                        whole spices       0.00      0.00      0.00         8\n",
      "                       cooking paste       0.00      0.00      0.00         7\n",
      "                      toilet cleaner       0.00      0.00      0.00         4\n",
      "                                 jam       0.00      0.00      0.00         6\n",
      "                            veg soup       0.00      0.00      0.00         6\n",
      "                             perfume       0.00      0.00      0.00         6\n",
      "                               papad       0.00      0.00      0.00         4\n",
      "                           baby soap       0.00      0.00      0.00         5\n",
      "                             noodles       0.00      0.00      0.00        19\n",
      "                      instant coffee       0.00      0.00      0.00         6\n",
      "pharmaceuticals and medical supplies       0.00      0.00      0.00         6\n",
      "                    toilet freshener       0.00      0.00      0.00         4\n",
      "                            cat food       0.00      0.00      0.00         4\n",
      "                              cheese       0.00      0.00      0.00         7\n",
      "                        insecticides       0.00      0.00      0.00         7\n",
      "                         corn flakes       0.00      0.00      0.00         5\n",
      "                   car air freshener       0.00      0.00      0.00         6\n",
      "                                oats       0.00      0.00      0.00         7\n",
      "               baby care accessories       0.00      0.00      0.00         5\n",
      "                                atta       0.00      0.00      0.00        16\n",
      "                             cookies       0.00      0.00      0.00         5\n",
      "                               dates       0.00      0.00      0.00         4\n",
      "                          toothbrush       0.00      0.00      0.00         6\n",
      "                         masala nuts       0.00      0.00      0.00         7\n",
      "\n",
      "                            accuracy                           0.54     26511\n",
      "                           macro avg       0.19      0.10      0.11     26511\n",
      "                        weighted avg       0.59      0.54      0.49     26511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test['category_id'], mnb_preds, target_names=test['category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uf5hmfCs_Uog"
   },
   "source": [
    "#### observation ####\n",
    "\n",
    "From the above results, the models is not at all performing in some classes. we need to fine tune in a better way. And removing the minority classes doesn't help at all in this cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY7CUgijATFX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "eda-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
