{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(12776, 27)\n",
      "Test Dataset Shape:(1530, 26)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset\n",
    "traindf = pd.read_csv('../FinalData/finaldata.csv')\n",
    "testdf = pd.read_csv('../FinalData/Testfinaldata.csv')\n",
    "\n",
    "print(\"Training Dataset Shape:{}\".format(traindf.shape))\n",
    "print(\"Test Dataset Shape:{}\".format(testdf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns In the dataset\n",
      "---------------\n",
      "['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'AwayTeam', 'FTR', 'HC', 'HF', 'HR', 'HS', 'HST', 'HTAG', 'HTHG', 'HY', 'HomeTeam', 'league', 'timestamp', 'ht_label', 'at_label', 'league_label', 'ftr_label', 'HTCT', 'ATCT', 'HTWP', 'ATWP']\n"
     ]
    }
   ],
   "source": [
    "columns = list(traindf.columns)\n",
    "print(\"Columns In the dataset\")\n",
    "print(\"-\" * 15)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Value Counts in the dataset\n",
      "H    5930\n",
      "A    3564\n",
      "D    3282\n",
      "Name: FTR, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_features = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY', \n",
    "                  'HC', 'HF', 'HR', 'HS', 'HST', 'HTAG', 'HTHG', 'HY',  \n",
    "                  'ht_label', 'at_label', 'league_label', 'HTCT', 'ATCT', 'HTWP', 'ATWP']\n",
    "\n",
    "label = 'ftr_label'\n",
    "\n",
    "#check the label value counts\n",
    "print(\"Total Value Counts in the dataset\")\n",
    "print(traindf.FTR.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Library called imbalanced-learn. To install it\n",
    "\n",
    "       pip install -U imbalanced-learn\n",
    "       \n",
    "       I will upsample the D class data using SMOTE algorithm. It creates synthetic observations of the minority class by\n",
    "\n",
    "    1. Finding K-nearest neighbors for minority class observations\n",
    "    2. Randomly choosing one of the K-nearest-neighbors and using to create a similar, but randomly tweaked, new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9280, 21), (9280,)]\n",
      "[(8352, 21), (928, 21), (8352,), (928,)]\n",
      "Training target statistics: Counter({1: 2942, 0: 2725, 2: 2685})\n",
      "Testing target statistics: Counter({1: 338, 2: 315, 0: 275})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import Counter\n",
    "#convert into train, test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_imbalance(traindf[input_features], traindf[label],\n",
    "                     sampling_strategy = {0:3000, 1:3280, 2:3000},\n",
    "                     random_state=12)\n",
    "print([X.shape, y.shape])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.1,\n",
    "    random_state=12)\n",
    "\n",
    "print([X_train.shape, X_test.shape, y_train.shape, y_test.shape])\n",
    "\n",
    "\n",
    "print('Training target statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing target statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "\n",
    "input_df = pd.DataFrame(X, columns=input_features)\n",
    "label_df = pd.DataFrame(y, columns=['ftr_label'])\n",
    "\n",
    "df = pd.concat([input_df, label_df], axis=1)\n",
    "df.to_csv(\"smoatdf.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training the Model\n",
      "Time Elapsed:0:00:06.556825\n",
      "---------------\n",
      "Model Metrics\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.63      0.67      0.83      0.65      0.75      0.55       275\n",
      "          1       0.54      0.49      0.76      0.51      0.61      0.36       338\n",
      "          2       0.66      0.69      0.82      0.67      0.75      0.56       315\n",
      "\n",
      "avg / total       0.61      0.61      0.80      0.61      0.70      0.48       928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb \n",
    "from datetime import datetime as dt\n",
    "\n",
    "#creating a pipeline\n",
    "pipeline = make_pipeline(NearMiss(version=2),\n",
    "                    xgb.XGBClassifier(seed = 12))\n",
    "\n",
    "start = dt.utcnow()\n",
    "print(\"Started Training the Model\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "end = dt.utcnow()\n",
    "\n",
    "print(\"Time Elapsed:{}\".format(end - start))\n",
    "print(\"-\" * 15)\n",
    "\n",
    "print(\"Model Metrics\")\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training the Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:0:00:04.945947\n",
      "---------------\n",
      "Model Metrics\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.61      0.67      0.82      0.64      0.74      0.54       275\n",
      "          1       0.56      0.43      0.81      0.49      0.59      0.34       338\n",
      "          2       0.63      0.73      0.78      0.68      0.76      0.57       315\n",
      "\n",
      "avg / total       0.60      0.60      0.80      0.60      0.69      0.48       928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=12,\n",
    "                                    solver='lbfgs', multi_class='ovr')\n",
    "\n",
    "#creating a pipeline\n",
    "pipeline2 = make_pipeline(NearMiss(version=2),\n",
    "                    clf)\n",
    "\n",
    "start = dt.utcnow()\n",
    "print(\"Started Training the Model\")\n",
    "pipeline2.fit(X_train, y_train)\n",
    "end = dt.utcnow()\n",
    "\n",
    "print(\"Time Elapsed:{}\".format(end - start))\n",
    "print(\"-\" * 15)\n",
    "\n",
    "print(\"Model Metrics\")\n",
    "print(classification_report_imbalanced(y_test, pipeline2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SMOTE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2725 2942 2685] [2725 2942 2942]\n",
      "Started Training the Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\madhivarman\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:0:00:04.704833\n",
      "---------------\n",
      "Model Metrics\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.61      0.67      0.82      0.64      0.74      0.54       275\n",
      "          1       0.57      0.45      0.81      0.51      0.60      0.35       338\n",
      "          2       0.64      0.73      0.79      0.68      0.76      0.57       315\n",
      "\n",
      "avg / total       0.61      0.61      0.80      0.60      0.70      0.48       928\n",
      "\n",
      "Started Training the Model\n",
      "Time Elapsed:0:00:05.298052\n",
      "---------------\n",
      "Model Metrics\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.58      0.69      0.79      0.63      0.74      0.54       275\n",
      "          1       0.51      0.61      0.66      0.55      0.63      0.40       338\n",
      "          2       0.81      0.50      0.94      0.62      0.69      0.45       315\n",
      "\n",
      "avg / total       0.63      0.60      0.79      0.60      0.68      0.46       928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE('minority')\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train), np.bincount(y_res))\n",
    "\n",
    "\n",
    "start = dt.utcnow()\n",
    "print(\"Started Training the Model\")\n",
    "pipeline2.fit(X_res, y_res)\n",
    "end = dt.utcnow()\n",
    "\n",
    "print(\"Time Elapsed:{}\".format(end - start))\n",
    "print(\"-\" * 15)\n",
    "\n",
    "print(\"Model Metrics\")\n",
    "print(classification_report_imbalanced(y_test, pipeline2.predict(X_test)))\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "lrclf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\n",
    "#creating a pipeline\n",
    "start = dt.utcnow()\n",
    "print(\"Started Training the Model\")\n",
    "pipeline4 = make_pipeline(NearMiss(version=2),\n",
    "                          lrclf)\n",
    "pipeline4.fit(X_train, y_train)\n",
    "end = dt.utcnow()\n",
    "\n",
    "print(\"Time Elapsed:{}\".format(end - start))\n",
    "print(\"-\" * 15)\n",
    "\n",
    "print(\"Model Metrics\")\n",
    "print(classification_report_imbalanced(y_test, pipeline4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction in Test Dataset ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimators ###\n",
    "\n",
    "A high level Tensorflow API that greatly simplify ML programming. It encapsulates following things\n",
    "\n",
    "1. Training\n",
    "2. Evaluating\n",
    "3. Prediction\n",
    "4. Export for Serving\n",
    "\n",
    "#### Structure of a Pre-Made Estimators Programs ####\n",
    "\n",
    "It typically consists of following 4 steps\n",
    "\n",
    "1. Convert CSV data into Tensorflow Records\n",
    "2. Define the Feature columns\n",
    "3. Create an relevant Algorithm\n",
    "4. Call a Training, Evaluation and Inference Method\n",
    "5. Export a serving function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape:(9280, 22)\n",
      "Test Dataset Shape:(1530, 26)\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('smoatdf.csv')\n",
    "test_data = pd.read_csv('../FinalData/Testfinaldata.csv')\n",
    "\n",
    "train_filename = 'smoatdf.csv'\n",
    "test_filename = '../FinalData/TestFinaldata.csv'\n",
    "\n",
    "\n",
    "print(\"Training Dataset Shape:{}\".format(training_data.shape))\n",
    "print(\"Test Dataset Shape:{}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv Input function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(features, labels, batch_size):\n",
    "    \n",
    "    #converts the inputs to dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    #shuffle\n",
    "    dataset =  dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \n",
    "    #input function for validation\n",
    "    features = dict(features)\n",
    "    \n",
    "    if labels is None:\n",
    "        #no labels only features\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "        \n",
    "    #convert inputs into dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    \n",
    "    assert batch_size is not None, \"Batch Size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Estimator ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT COLUMNS\n",
    "important_columns = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'HC', 'HF',\n",
    "       'HR', 'HS', 'HST', 'HTAG', 'HTHG', 'HY','ht_label', 'at_label', 'league_label', \n",
    "        'HTCT', 'ATCT','HTWP', 'ATWP']\n",
    "\n",
    "\n",
    "#get input and output features\n",
    "X_all = training_data[important_columns]\n",
    "y_all = training_data['ftr_label']\n",
    "\n",
    "# Shuffle and split the dataset into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n",
    "                                                    test_size = 1500,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature columns\n",
    "my_feature_columns = []\n",
    "\n",
    "for key in important_columns:\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\n",
      "INFO:tensorflow:Using config: {'_device_fn': None, '_log_step_count_steps': 100, '_is_chief': True, '_model_dir': 'C:\\\\Users\\\\MADHIV~1\\\\AppData\\\\Local\\\\Temp\\\\tmpcg5cixcb', '_global_id_in_cluster': 0, '_task_type': 'worker', '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_save_summary_steps': 100, '_session_config': None, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025B10462630>, '_keep_checkpoint_max': 5, '_task_id': 0, '_master': '', '_train_distribute': None, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt.\n",
      "INFO:tensorflow:loss = 127.11543, step = 0\n",
      "INFO:tensorflow:global_step/sec: 182.636\n",
      "INFO:tensorflow:loss = 17.831688, step = 100 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.335\n",
      "INFO:tensorflow:loss = 17.679504, step = 200 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.197\n",
      "INFO:tensorflow:loss = 17.310822, step = 300 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.147\n",
      "INFO:tensorflow:loss = 17.580215, step = 400 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.606\n",
      "INFO:tensorflow:loss = 17.728617, step = 500 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.921\n",
      "INFO:tensorflow:loss = 17.216871, step = 600 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.554\n",
      "INFO:tensorflow:loss = 15.19777, step = 700 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.93\n",
      "INFO:tensorflow:loss = 14.514133, step = 800 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.033\n",
      "INFO:tensorflow:loss = 17.155163, step = 900 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.771\n",
      "INFO:tensorflow:loss = 10.803084, step = 1000 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.336\n",
      "INFO:tensorflow:loss = 9.842501, step = 1100 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.723\n",
      "INFO:tensorflow:loss = 18.53774, step = 1200 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.661\n",
      "INFO:tensorflow:loss = 11.982333, step = 1300 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.899\n",
      "INFO:tensorflow:loss = 15.736992, step = 1400 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.112\n",
      "INFO:tensorflow:loss = 9.382629, step = 1500 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.486\n",
      "INFO:tensorflow:loss = 9.954208, step = 1600 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.513\n",
      "INFO:tensorflow:loss = 13.930011, step = 1700 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.09\n",
      "INFO:tensorflow:loss = 17.140242, step = 1800 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.84\n",
      "INFO:tensorflow:loss = 16.896393, step = 1900 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.336\n",
      "INFO:tensorflow:loss = 12.738801, step = 2000 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.318\n",
      "INFO:tensorflow:loss = 12.784474, step = 2100 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.767\n",
      "INFO:tensorflow:loss = 11.924512, step = 2200 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.289\n",
      "INFO:tensorflow:loss = 15.921425, step = 2300 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.443\n",
      "INFO:tensorflow:loss = 11.377453, step = 2400 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.929\n",
      "INFO:tensorflow:loss = 15.634007, step = 2500 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.75\n",
      "INFO:tensorflow:loss = 13.619556, step = 2600 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.7\n",
      "INFO:tensorflow:loss = 9.819517, step = 2700 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.842\n",
      "INFO:tensorflow:loss = 10.589236, step = 2800 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.438\n",
      "INFO:tensorflow:loss = 9.8846855, step = 2900 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.754\n",
      "INFO:tensorflow:loss = 14.4999695, step = 3000 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.543\n",
      "INFO:tensorflow:loss = 8.451405, step = 3100 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.841\n",
      "INFO:tensorflow:loss = 14.573673, step = 3200 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.944\n",
      "INFO:tensorflow:loss = 15.43123, step = 3300 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.094\n",
      "INFO:tensorflow:loss = 11.547823, step = 3400 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.296\n",
      "INFO:tensorflow:loss = 15.837596, step = 3500 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.42\n",
      "INFO:tensorflow:loss = 12.909811, step = 3600 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.191\n",
      "INFO:tensorflow:loss = 12.207472, step = 3700 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.867\n",
      "INFO:tensorflow:loss = 15.416166, step = 3800 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.485\n",
      "INFO:tensorflow:loss = 10.754162, step = 3900 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.146\n",
      "INFO:tensorflow:loss = 13.493385, step = 4000 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.842\n",
      "INFO:tensorflow:loss = 16.875803, step = 4100 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.841\n",
      "INFO:tensorflow:loss = 9.251432, step = 4200 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.251\n",
      "INFO:tensorflow:loss = 10.5786915, step = 4300 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.664\n",
      "INFO:tensorflow:loss = 14.121743, step = 4400 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.297\n",
      "INFO:tensorflow:loss = 9.604394, step = 4500 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.199\n",
      "INFO:tensorflow:loss = 10.615437, step = 4600 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.84\n",
      "INFO:tensorflow:loss = 11.82513, step = 4700 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.514\n",
      "INFO:tensorflow:loss = 12.176163, step = 4800 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.871\n",
      "INFO:tensorflow:loss = 15.488007, step = 4900 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.42\n",
      "INFO:tensorflow:loss = 13.906134, step = 5000 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.85\n",
      "INFO:tensorflow:loss = 7.217288, step = 5100 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.785\n",
      "INFO:tensorflow:loss = 13.622097, step = 5200 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.842\n",
      "INFO:tensorflow:loss = 9.535442, step = 5300 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.095\n",
      "INFO:tensorflow:loss = 12.942682, step = 5400 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.201\n",
      "INFO:tensorflow:loss = 10.934145, step = 5500 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.841\n",
      "INFO:tensorflow:loss = 9.800995, step = 5600 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.609\n",
      "INFO:tensorflow:loss = 9.495125, step = 5700 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.276\n",
      "INFO:tensorflow:loss = 11.98666, step = 5800 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.201\n",
      "INFO:tensorflow:loss = 12.386417, step = 5900 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.133\n",
      "INFO:tensorflow:loss = 8.660218, step = 6000 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.868\n",
      "INFO:tensorflow:loss = 11.044134, step = 6100 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.404\n",
      "INFO:tensorflow:loss = 12.083487, step = 6200 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.758\n",
      "INFO:tensorflow:loss = 11.613367, step = 6300 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.862\n",
      "INFO:tensorflow:loss = 13.342684, step = 6400 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.624\n",
      "INFO:tensorflow:loss = 7.2324376, step = 6500 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.199\n",
      "INFO:tensorflow:loss = 13.632562, step = 6600 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.701\n",
      "INFO:tensorflow:loss = 13.766109, step = 6700 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.65\n",
      "INFO:tensorflow:loss = 17.57044, step = 6800 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.134\n",
      "INFO:tensorflow:loss = 14.79781, step = 6900 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.669\n",
      "INFO:tensorflow:loss = 10.533106, step = 7000 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.842\n",
      "INFO:tensorflow:loss = 13.449425, step = 7100 (0.394 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 251.172\n",
      "INFO:tensorflow:loss = 15.135541, step = 7200 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.384\n",
      "INFO:tensorflow:loss = 14.111376, step = 7300 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.2\n",
      "INFO:tensorflow:loss = 10.052315, step = 7400 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.784\n",
      "INFO:tensorflow:loss = 12.641413, step = 7500 (0.391 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7600 into C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 10.081821.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-18-09:19:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt-7600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-18-09:19:44\n",
      "INFO:tensorflow:Saving dict for global step 7600: accuracy = 0.6333333, average_loss = 0.8089081, global_step = 7600, loss = 6.4540544\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7600: C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt-7600\n",
      "Test Accuracy: 0.633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns = my_feature_columns,\n",
    "    #three hidden layers of 30 nodes each\n",
    "    hidden_units = [30, 30, 10],\n",
    "    #model must choose between 3 classes\n",
    "    n_classes=3\n",
    ")\n",
    "\n",
    "#train the model\n",
    "training_result = classifier.train(\n",
    "    input_fn = lambda: csv_input_fn(X_train,y_train, 16),\n",
    "    steps = 7600\n",
    ")\n",
    "\n",
    "\n",
    "#evaluate the model\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn = lambda: eval_input_fn(X_test, y_test, 8)\n",
    ")\n",
    "\n",
    "print(\"Test Accuracy:{accuracy: 0.3f}\\n\".format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check down the model performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldfinp = pd.DataFrame(X_test, columns=important_columns)\n",
    "evaldfinp.to_csv(\"evaldf.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe into json list file\n",
    "evaldf = pd.read_csv(\"evaldf.csv\")\n",
    "\n",
    "cols = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'HC', 'HF',\n",
    "       'HR', 'HS', 'HST', 'HTAG', 'HTHG', 'HY','ht_label', 'at_label', 'league_label', \n",
    "        'HTCT', 'ATCT','HTWP', 'ATWP']\n",
    "\n",
    "impo_test_data = evaldf[cols]\n",
    "\n",
    "predict_y = {}\n",
    "\n",
    "for cols in list(impo_test_data.columns):\n",
    "    val = list(impo_test_data[cols].values)\n",
    "    predict_y.update({cols:val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['HY', 'ht_label', 'HST', 'HS', 'HC', 'at_label', 'HF', 'league_label', 'AY', 'AST', 'AF', 'HTHG', 'ATWP', 'HTCT', 'AC', 'AS', 'ATCT', 'HTAG', 'HTWP', 'HR', 'AR'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt-7600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "expected = ['A', 'D', 'H']\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn = lambda : eval_input_fn(predict_y, labels=None, batch_size=1)\n",
    ")\n",
    "\n",
    "template = ('\\n Prediction is \"{}\" ({:1f}%)')\n",
    "who_win, only_class_pred = [], [] #list to store\n",
    "\n",
    "for pred in predictions:\n",
    "    class_id = pred['class_ids'][0]\n",
    "    probability = pred['probabilities'][class_id]\n",
    "    \n",
    "    #print(template.format(expected[class_id], 100 * probability))\n",
    "    only_class_pred.append(class_id)\n",
    "    who_win.append([expected[class_id], 100 * probability])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 --> Away, 1 --> DrawTeam,  2 --> HomeTeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Stability on Validation Test Set ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.72      0.62      0.89      0.67      0.74      0.53       485\n",
      "          1       0.53      0.62      0.70      0.57      0.66      0.43       530\n",
      "          2       0.70      0.67      0.86      0.68      0.76      0.56       485\n",
      "\n",
      "avg / total       0.64      0.63      0.81      0.64      0.72      0.51      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, only_class_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model stability in Test dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\MADHIV~1\\AppData\\Local\\Temp\\tmpcg5cixcb\\model.ckpt-7600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1530, 2)\n"
     ]
    }
   ],
   "source": [
    "cols_to_consider = ['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'HC', 'HF',\n",
    "       'HR', 'HS', 'HST', 'HTAG', 'HTHG', 'HY','ht_label', 'at_label', 'league_label', \n",
    "        'HTCT', 'ATCT','HTWP', 'ATWP']\n",
    "\n",
    "testdf = test_data[cols_to_consider]\n",
    "test = {}\n",
    "modelprediction = []\n",
    "\n",
    "for cols in list(testdf.columns):\n",
    "    val= list(testdf[cols].values)\n",
    "    test.update({cols:val})\n",
    "\n",
    "testPredictions = classifier.predict(\n",
    "    input_fn = lambda : eval_input_fn(test, labels=None, batch_size=1)\n",
    ")\n",
    "\n",
    "for pred in testPredictions:\n",
    "    class_id = pred['class_ids'][0]\n",
    "    probability = pred['probabilities'][class_id]\n",
    "    modelprediction.append([expected[class_id], 100 * probability])\n",
    "    \n",
    "    \n",
    "preddf = pd.DataFrame(modelprediction, columns=['Prediction', 'Probability'])\n",
    "print(preddf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtestdf = pd.read_csv(\"../data/test.csv\")\n",
    "concatdf = pd.concat([test_data, preddf], axis=1)\n",
    "concatdf.shape\n",
    "concatdf.to_csv(\"output.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Stability Performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(612, 64), (1530, 28)]\n"
     ]
    }
   ],
   "source": [
    "#to check the performance i downloaded the data from net to cross check(2017, 2018) matches\n",
    "d1 = pd.read_csv(\"../MatchResults/D1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.84      0.58      0.96      0.69      0.75      0.54        84\n",
      "          D       0.41      0.49      0.74      0.45      0.60      0.36        83\n",
      "          H       0.73      0.78      0.76      0.76      0.77      0.60       139\n",
      "\n",
      "avg / total       0.68      0.65      0.81      0.66      0.72      0.52       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predd1 = outputdf.loc[:305]\n",
    "mpred = list(predd1.Prediction)\n",
    "actresult = list(d1.FTR)\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(actresult, mpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Model Prediction(SGD Classifier) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.87      0.55      0.97      0.67      0.73      0.51        84\n",
      "          D       0.43      0.69      0.66      0.53      0.67      0.45        83\n",
      "          H       0.80      0.69      0.86      0.74      0.77      0.58       139\n",
      "\n",
      "avg / total       0.72      0.65      0.83      0.66      0.73      0.53       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdata = testdf.values\n",
    "sk_prediction = lrclf.predict(testdata)\n",
    "\n",
    "expected = ['A', 'D', 'H']\n",
    "asteam = []\n",
    "\n",
    "convertedList = [expected[x] for x in sk_prediction]\n",
    "#take only first 305 rows\n",
    "split = convertedList[:306]\n",
    "\n",
    "test_data['output'] = convertedList\n",
    "test_data.to_csv('sklearnOutput.csv', sep=\",\", index=False)\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(actresult, split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Stability Improvement ####\n",
    "\n",
    "Model stability improved from\n",
    "\n",
    "| Team | Pre  | Rec | f1 |\n",
    "|------|------|-----|----|\n",
    "|   0  | 0.83 |0.60 |0.70|\n",
    "|   1  | 0.44 |0.34 |0.38|\n",
    "|   2  | 0.66 |0.86 |0.75|\n",
    "\n",
    "\n",
    "        \n",
    "                                                                to\n",
    "    \n",
    "| Team | Pre  | Rec | f1 |\n",
    "|------|------|-----|----|\n",
    "|   A  | 0.87 |0.55 |0.67|\n",
    "|   D  | 0.43 |0.69 |0.53|\n",
    "|   H  | 0.80 |0.69 |0.74|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
